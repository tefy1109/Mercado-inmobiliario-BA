# An√°lisis del Mercado Inmobiliario de Buenos Aires

Este proyecto realiza web scraping y an√°lisis de datos del mercado inmobiliario de Buenos Aires, Argentina, utilizando t√©cnicas de extracci√≥n automatizada para obtener informaci√≥n de propiedades en alquiler desde ZonaProp.

## üìÅ Estructura del Proyecto

```
Mercado-inmobiliario-BA/
‚îú‚îÄ‚îÄ selenium_zonaprop.py           # Script principal de web scraping
‚îú‚îÄ‚îÄ output/                       # Datos extra√≠dos (JSON y CSV)
‚îú‚îÄ‚îÄ etl/                         # limpieza, tratado y extracci√≥n
‚îú‚îÄ‚îÄ data/                         # Datos procesados
‚îú‚îÄ‚îÄ powerBI/                         # analisis de datos
‚îî‚îÄ‚îÄ README.md                     # Documentaci√≥n del proyecto
```

## üéØ Objetivos del Proyecto

- **Web Scraping Avanzado**: Extracci√≥n automatizada de datos inmobiliarios de ZonaProp
- **An√°lisis de Precios**: Obtener informaci√≥n detallada de precios de alquiler
- **Datos Estructurados**: Generar datasets en formatos JSON y CSV
- **Escalabilidad**: Procesamiento de m√∫ltiples p√°ginas con manejo de errores

## üõ† Caracter√≠sticas del Scraper

### Funcionalidades Principales

- ‚úÖ **Anti-detecci√≥n**: User-agents rotativos y comportamiento humano simulado
- ‚úÖ **Manejo de errores robusto**: Reintentos autom√°ticos y recovery
- ‚úÖ **M√∫ltiples p√°ginas**: Navegaci√≥n autom√°tica entre p√°ginas de resultados
- ‚úÖ **Captcha handling**: Detecci√≥n y pausa para resoluci√≥n manual
- ‚úÖ **Navegaci√≥n natural**: Simulaci√≥n de b√∫squedas en Google antes del scraping
- ‚úÖ **Delays aleatorios**: Tiempos de espera variables para evitar detecci√≥n
- ‚úÖ **Screenshots debug**: Capturas autom√°ticas para depuraci√≥n

### Datos Extra√≠dos

El scraper extrae la siguiente informaci√≥n de cada propiedad:

```json
{
  "precio_alquiler": 150000,
  "expensas": 25000,
  "direccion": "Av. Rivadavia 7500, Flores",
  "zona": "Flores",
  "superficie": 45,
  "ambientes": 2,
  "habitaciones": 1,
  "banos": 1,
  "descripcion": "Departamento 2 ambientes en Flores",
  "url": "https://www.zonaprop.com.ar/...",
  "pagina": 1,
  "scraped_at": "2024-01-15T10:30:00"
}
```

## üöÄ Instalaci√≥n y Configuraci√≥n

### Requisitos Previos

- **Python 3.7+**
- **Google Chrome** (√∫ltima versi√≥n)
- **ChromeDriver** (autom√°ticamente gestionado por Selenium)

### Dependencias

```bash
pip install selenium
pip install webdriver-manager  # Opcional, para gesti√≥n autom√°tica de drivers
```

### Dependencias del Sistema

```bash
# Ubuntu/Debian
sudo apt-get update
sudo apt-get install -y google-chrome-stable

# macOS (con Homebrew)
brew install --cask google-chrome

# Windows
# Descargar e instalar Chrome desde https://www.google.com/chrome/
```

## üéÆ Uso del Scraper

### Ejecuci√≥n B√°sica

```bash
python selenium_zonaprop.py
```

### Configuraci√≥n Personalizada

El script incluye par√°metros configurables en la funci√≥n `main()`:

```python
# Modificar estos valores seg√∫n necesidades
base_url = 'https://www.zonaprop.com.ar/departamentos-alquiler-flores.html'
max_retries = 3      # Reintentos por p√°gina
max_pages = 15       # M√°ximo de p√°ginas a procesar
```

### Personalizar Zona de B√∫squeda

Para cambiar la zona de b√∫squeda, modificar la `base_url`:

```python
# Ejemplos de URLs para diferentes zonas
base_url = 'https://www.zonaprop.com.ar/departamentos-alquiler-palermo.html'
base_url = 'https://www.zonaprop.com.ar/departamentos-alquiler-recoleta.html'
base_url = 'https://www.zonaprop.com.ar/departamentos-alquiler-belgrano.html'
```

## üìä Estructura de Datos

### Archivos de Salida

El scraper genera dos tipos de archivos con timestamp:

- **JSON**: `output/zonaprop_propiedades_YYYYMMDD_HHMMSS.json`
- **CSV**: `output/zonaprop_propiedades_YYYYMMDD_HHMMSS.csv`

### Campos de Datos

| Campo | Tipo | Descripci√≥n |
|-------|------|-------------|
| `precio_alquiler` | Integer | Precio mensual de alquiler (ARS) |
| `expensas` | Integer | Expensas mensuales (ARS) |
| `direccion` | String | Direcci√≥n completa de la propiedad |
| `zona` | String | Barrio/zona de ubicaci√≥n |
| `superficie` | Integer | Superficie en metros cuadrados |
| `ambientes` | Integer | Cantidad de ambientes |
| `habitaciones` | Integer | Cantidad de habitaciones |
| `banos` | Integer | Cantidad de ba√±os |
| `descripcion` | String | T√≠tulo/descripci√≥n de la propiedad |
| `url` | String | URL completa de la propiedad |
| `pagina` | Integer | N√∫mero de p√°gina donde se encontr√≥ |
| `scraped_at` | String | Timestamp de extracci√≥n (ISO format) |

## üîß Caracter√≠sticas T√©cnicas

### Configuraci√≥n del WebDriver

```python
# Opciones anti-detecci√≥n
chrome_options.add_argument("--disable-blink-features=AutomationControlled")
chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
chrome_options.add_experimental_option('useAutomationExtension', False)

# Timeouts aumentados
driver.set_page_load_timeout(180)  # 3 minutos
driver.set_script_timeout(180)     # 3 minutos
```

### Selectores CSS Utilizados

El scraper utiliza m√∫ltiples selectores de respaldo para mayor robustez:

```python
price_selectors = [
    'div.postingCard-module__price-container div:first-child',
    'div[data-qa="POSTING_CARD_PRICE"]',
    'div.price-data',
    'div.postingPrice'
]
```

### Estrategias Anti-Detecci√≥n

1. **User-Agents Rotativos**: 4 user-agents diferentes
2. **Navegaci√≥n Natural**: B√∫squeda previa en Google
3. **Delays Humanos**: Tiempos aleatorios entre acciones
4. **Scroll Suave**: Simulaci√≥n de comportamiento de lectura
5. **Tipeo Simulado**: Entrada de texto car√°cter por car√°cter

## üêõ Depuraci√≥n y Monitoreo

### Archivos de Debug

- `captcha_detected.png`: Screenshot cuando se detecta CAPTCHA
- `debug_page.html`: HTML de p√°gina para an√°lisis
- `error_page_X_attempt_Y.html`: HTML de p√°ginas con error

### Logging

El script incluye logging detallado:

```
‚úì Directorio output creado/verificado
Configurando navegador Chrome...
Procesando p√°gina 1: https://www.zonaprop.com.ar/...
Intento 1 de 3 para la p√°gina 1
Visitando Google primero...
Encontrados 20 propiedades con selector: div.postingCard
Propiedad extra√≠da: Av. Rivadavia 7500 - $150000
‚úÖ P√°gina 1 scrapeada exitosamente. 20 propiedades extra√≠das.
```

## ‚ö†Ô∏è Consideraciones Legales y √âticas

- **Respeto a robots.txt**: Verificar t√©rminos de uso de ZonaProp
- **Uso responsable**: No sobrecargar los servidores
- **Datos personales**: No almacenar informaci√≥n personal identificable
- **Prop√≥sito educativo**: Este proyecto es para an√°lisis y aprendizaje

## üîÆ Mejoras Futuras

- [ ] **Paralelizaci√≥n**: M√∫ltiples instancias de navegador
- [ ] **Base de datos**: Almacenamiento en PostgreSQL/MongoDB
- [ ] **API REST**: Endpoint para consultar datos
- [ ] **An√°lisis temporal**: Tracking de cambios de precios
- [ ] **Notificaciones**: Alertas de nuevas propiedades
- [ ] **Filtros avanzados**: Por precio, superficie, etc.
- [ ] **Visualizaciones**: Mapas de calor y gr√°ficos
- [ ] **ML Models**: Predicci√≥n de precios

## ü§ù Contribuciones

Las contribuciones son bienvenidas. Para contribuir:

1. Fork el proyecto
2. Crea una rama feature (`git checkout -b feature/mejora-scraper`)
3. Commit cambios (`git commit -m 'Agregar nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/mejora-scraper`)
5. Abre un Pull Request

## üìä Estad√≠sticas del Proyecto

- **L√≠neas de c√≥digo**: ~400 l√≠neas
- **Selectores CSS**: 15+ selectores de respaldo
- **Campos extra√≠dos**: 12 campos por propiedad
- **Formatos de salida**: JSON y CSV
- **Zonas soportadas**: Todas las disponibles en ZonaProp

## üÜò Troubleshooting

### Problemas Comunes

1. **ChromeDriver no encontrado**
   ```bash
   pip install webdriver-manager
   ```

2. **Timeout en carga de p√°gina**
   - Verificar conexi√≥n a internet
   - Aumentar timeout en el c√≥digo

3. **CAPTCHA frecuentes**
   - Reducir velocidad de scraping
   - Cambiar user-agent
   - Usar proxy/VPN

4. **Selectores no funcionan**
   - ZonaProp cambi√≥ la estructura
   - Actualizar selectores CSS
   - Revisar archivos debug HTML

## üìß Contacto

Para preguntas, sugerencias o reportar bugs, crear un issue en este repositorio.

